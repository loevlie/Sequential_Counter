{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GradCAM Visualization for VLMs\n",
    "\n",
    "This notebook explains how we use Gradient-weighted Class Activation Mapping (GradCAM) to visualize which image regions the Vision-Language Model focuses on during counting.\n",
    "\n",
    "## What is GradCAM?\n",
    "\n",
    "GradCAM is a technique for visualizing where a neural network is \"looking\" when making predictions. It combines:\n",
    "1. **Gradients**: How much the output changes with respect to feature maps\n",
    "2. **Activations**: The actual feature representations from the model\n",
    "\n",
    "For VLMs like Qwen3-VL, GradCAM shows us which image regions influence the model's counting prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why GradCAM for VLMs?\n",
    "\n",
    "Vision-Language Models like Qwen3-VL use a **decoder-only architecture** (similar to GPT with vision):\n",
    "- Image and text tokens are in the same sequence\n",
    "- Only self-attention is available (no cross-attention)\n",
    "- Traditional attention visualization doesn't work well\n",
    "\n",
    "GradCAM solves this by:\n",
    "- Using gradients from the backward pass\n",
    "- Weighting feature activations by their importance\n",
    "- Creating spatial heatmaps showing region importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import our GradCAM module\n",
    "from visualize_vlm_gradcam import VLMGradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How GradCAM Works\n",
    "\n",
    "### Step-by-Step Process\n",
    "\n",
    "1. **Forward Pass**: Run image through VLM to get count prediction\n",
    "   ```python\n",
    "   outputs = model(**inputs)\n",
    "   logits = outputs.logits\n",
    "   score = logits[0, -1, :].max()  # Model's confidence\n",
    "   ```\n",
    "\n",
    "2. **Register Hooks**: Capture activations and gradients from vision encoder\n",
    "   ```python\n",
    "   def forward_hook(module, input, output):\n",
    "       self.activations = output  # Save activations\n",
    "   \n",
    "   def backward_hook(module, grad_input, grad_output):\n",
    "       self.gradients = grad_output[0]  # Save gradients\n",
    "   ```\n",
    "\n",
    "3. **Backward Pass**: Compute gradients w.r.t. the output\n",
    "   ```python\n",
    "   score.backward()  # Backpropagate\n",
    "   ```\n",
    "\n",
    "4. **Compute GradCAM**: Weight activations by gradients\n",
    "   ```python\n",
    "   # Pool gradients spatially\n",
    "   pooled_grads = gradients.mean(dim=0)\n",
    "   \n",
    "   # Weight activations\n",
    "   weighted_acts = activations * pooled_grads\n",
    "   \n",
    "   # Average across channels and apply ReLU\n",
    "   heatmap = weighted_acts.mean(dim=-1)\n",
    "   heatmap = ReLU(heatmap)  # Keep only positive contributions\n",
    "   ```\n",
    "\n",
    "5. **Create Visualization**: Resize heatmap to image size and overlay\n",
    "   ```python\n",
    "   # Normalize heatmap\n",
    "   heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "   \n",
    "   # Resize to image dimensions\n",
    "   heatmap_resized = resize(heatmap, (img_height, img_width))\n",
    "   \n",
    "   # Overlay on image\n",
    "   plt.imshow(image)\n",
    "   plt.imshow(heatmap_resized, cmap='jet', alpha=0.5)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Details\n",
    "\n",
    "### Vision Tokenization\n",
    "\n",
    "Qwen3-VL divides images into patches:\n",
    "- Full image (384×512): ~864 vision tokens → 24×36 grid\n",
    "- Crop (256×256): ~280 vision tokens → 14×20 grid\n",
    "\n",
    "### Tensor Shapes\n",
    "\n",
    "During GradCAM computation:\n",
    "```\n",
    "Gradients:   [num_tokens, hidden_dim]  e.g., [864, 1024]\n",
    "Activations: [num_tokens, hidden_dim]  e.g., [864, 1024]\n",
    "```\n",
    "\n",
    "We need to:\n",
    "1. Pool gradients over hidden dimension\n",
    "2. Weight activations\n",
    "3. Reshape token sequence to 2D spatial grid\n",
    "4. Resize to original image dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the GradCAM Visualizer\n",
    "\n",
    "### Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_with_gradcam(image_path, category, strategy='comparison'):\n    \"\"\"\n    Generate GradCAM visualizations for an image.\n    \n    Args:\n        image_path: Path to image\n        category: Object category to count\n        strategy: 'comparison', 'dense', or 'hybrid'\n    \"\"\"\n    # Initialize GradCAM\n    gradcam = VLMGradCAM()\n    \n    # Load image\n    image = Image.open(image_path)\n    \n    # Generate visualization using the unified API\n    output_path = f'output_{strategy}.png'\n    gradcam.visualize_counting_attention(image, category, strategy=strategy, output_path=output_path)\n    \n    print(f\"Visualization saved to {output_path}!\")\n\n# Example usage (uncomment to run)\n# visualize_with_gradcam('/media/M2SSD/FSC147/images_384_VarV2/194.jpg', 'peaches', 'hybrid')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting GradCAM Heatmaps\n",
    "\n",
    "### Color Scale\n",
    "- **Red/Yellow**: High importance (VLM focused here)\n",
    "- **Green/Blue**: Medium importance\n",
    "- **Purple/Dark**: Low importance (VLM ignored)\n",
    "\n",
    "### What Good Heatmaps Look Like\n",
    "\n",
    "For accurate counting:\n",
    "- Red/yellow regions should align with object locations\n",
    "- Multiple objects should have multiple hotspots\n",
    "- Background should be low activation (blue/purple)\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "1. **Well-distributed objects**: Heatmap shows multiple distinct hotspots\n",
    "2. **Clustered objects**: Heatmap shows larger connected regions\n",
    "3. **Edge objects**: May have lower activation (boundary artifacts)\n",
    "4. **Occlusion**: Partially visible objects may have weaker activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Visualizations\n",
    "\n",
    "Let's look at the example visualizations we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example GradCAM visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Load and display the three example visualizations\n",
    "viz_dir = Path('../visualizations/gradcam')\n",
    "\n",
    "images = [\n",
    "    'gradcam_comparison_3.png',\n",
    "    'gradcam_dense_3.png', \n",
    "    'gradcam_hybrid_3.png'\n",
    "]\n",
    "\n",
    "titles = [\n",
    "    'Comparison: Global vs Crop',\n",
    "    'Dense Grid: 3×3',\n",
    "    'Hybrid: Global + Quadrants'\n",
    "]\n",
    "\n",
    "for ax, img_name, title in zip(axes, images, titles):\n",
    "    img_path = viz_dir / img_name\n",
    "    if img_path.exists():\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, f'Run visualizations first:\\n{img_name}',\n",
    "               ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Notes\n",
    "\n",
    "### Challenges Solved\n",
    "\n",
    "1. **Tensor Shape Handling**\n",
    "   - Vision tokens are 1D sequences that need to be reshaped to 2D grids\n",
    "   - Not all token counts are perfect squares (e.g., 864 = 24×36)\n",
    "   - Solution: Try common aspect ratios and grid sizes\n",
    "\n",
    "2. **Float16 Compatibility**\n",
    "   - Scipy's `gaussian_filter` doesn't support float16\n",
    "   - Solution: Convert to float32 before smoothing operations\n",
    "\n",
    "3. **Hook Registration**\n",
    "   - Need to capture both forward activations and backward gradients\n",
    "   - Solution: Register both forward and backward hooks on vision blocks\n",
    "\n",
    "### Code Structure\n",
    "\n",
    "```python\n",
    "class VLMGradCAM:\n",
    "    def __init__(self, model_name):\n",
    "        # Load VLM\n",
    "        # Initialize hook storage\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        # Register forward/backward hooks on vision encoder\n",
    "    \n",
    "    def generate_gradcam(self, image, category, crop_bbox=None):\n",
    "        # 1. Forward pass\n",
    "        # 2. Backward pass  \n",
    "        # 3. Compute GradCAM from gradients + activations\n",
    "        # 4. Return heatmap\n",
    "    \n",
    "    def visualize_comparison(self, image, category, output_path):\n",
    "        # Generate global and crop GradCAM\n",
    "        # Create side-by-side visualization\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "### 1. Debugging Counting Errors\n",
    "- If count is wrong, check if heatmap covers all objects\n",
    "- Missing hotspots → objects not detected by VLM\n",
    "- Background activation → VLM confused by clutter\n",
    "\n",
    "### 2. Validating Strategies\n",
    "- Compare global vs local heatmaps\n",
    "- Ensure crops capture relevant regions\n",
    "- Verify overlap handling in grid strategies\n",
    "\n",
    "### 3. Model Analysis\n",
    "- Understand VLM counting behavior\n",
    "- Identify failure modes (edge cases, occlusion)\n",
    "- Guide improvements to counting strategies\n",
    "\n",
    "### 4. Research & Development\n",
    "- Visualize attention for RL reward signals\n",
    "- Analyze which image regions contribute to predictions\n",
    "- Debug VLM-based RL agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Custom GradCAM\n",
    "\n",
    "You can create custom visualizations by directly using the GradCAM generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_gradcam_viz(image_path, category, custom_crops):\n",
    "    \"\"\"\n",
    "    Create custom GradCAM visualization with specific crop regions.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image\n",
    "        category: Object category\n",
    "        custom_crops: List of (x1, y1, x2, y2) crop boxes\n",
    "    \"\"\"\n",
    "    gradcam = VLMGradCAM()\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(custom_crops) + 1, figsize=(6*(len(custom_crops)+1), 6))\n",
    "    \n",
    "    # Global view\n",
    "    heatmap = gradcam.generate_gradcam(image, category)\n",
    "    axes[0].imshow(image)\n",
    "    if heatmap is not None:\n",
    "        axes[0].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    axes[0].set_title('Global', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Custom crops\n",
    "    for i, crop_bbox in enumerate(custom_crops):\n",
    "        heatmap = gradcam.generate_gradcam(image, category, crop_bbox)\n",
    "        \n",
    "        # Show crop\n",
    "        cropped = image.crop(crop_bbox)\n",
    "        axes[i+1].imshow(cropped)\n",
    "        if heatmap is not None:\n",
    "            axes[i+1].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "        axes[i+1].set_title(f'Crop {i+1}', fontsize=14, fontweight='bold')\n",
    "        axes[i+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example: Visualize specific regions of interest\n",
    "# custom_crops = [(0, 0, 192, 192), (192, 0, 384, 192), (0, 192, 192, 384)]\n",
    "# fig = custom_gradcam_viz('/path/to/image.jpg', 'objects', custom_crops)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- **GradCAM Paper**: \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\" (Selvaraju et al., 2017)\n",
    "- **VLM Architecture**: Qwen3-VL uses decoder-only architecture with vision patches\n",
    "- **Alternative Methods**: We also explored Relevancy Propagation (Chefer et al.) but GradCAM proved more practical\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Return to `01_VLM_Counting_Strategies.ipynb` to learn about counting strategies\n",
    "- Explore source code: `src/visualize_vlm_gradcam.py` for full implementation\n",
    "- Run evaluations: `src/evaluate_all_methods.py` to test on FSC147 dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}